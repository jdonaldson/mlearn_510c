{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sharing-failing",
   "metadata": {},
   "source": [
    "# Deployment (Including Serialization)\n",
    "This notebook walks through the basics of how to set up a model to be served from a webserver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "single-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-attack",
   "metadata": {},
   "source": [
    "We can use the `joblib` library to deserialize the serialized pipeline.  HOWEVER... we need to make sure we have loaded all of the transformer classes into the scope here, or else deserialization will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "given-egypt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipe = joblib.load(\"train_pipe.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-heart",
   "metadata": {},
   "source": [
    "I've put all the relevant transformers in a separate script called `pipeline.py`, and we can import them all in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unusual-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cheap-eating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BaseEstimator',\n",
       " 'DateTimeExpander',\n",
       " 'FeatureSelector',\n",
       " 'FeelsLikeExpander',\n",
       " 'LagExpander',\n",
       " 'TargetDropper',\n",
       " 'Temp',\n",
       " 'TransformerMixin',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'feels_like',\n",
       " 'pd']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pipeline\n",
    "dir(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "personalized-gothic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\r\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
      "\r\n",
      "class FeatureSelector(BaseEstimator, TransformerMixin):\r\n",
      "\r\n",
      "    def __init__(self, feature_names, ts_index):\r\n",
      "        self.feature_names = feature_names\r\n",
      "        self.index = ts_index\r\n",
      "\r\n",
      "    def fit(self, X, y=None):\r\n",
      "        return self\r\n",
      "\r\n",
      "    def transform(self, X):\r\n",
      "        X = X.set_index(pd.to_datetime(X[self.index]))\r\n",
      "        return X[self.feature_names]\r\n",
      "\r\n",
      "class DateTimeExpander(BaseEstimator, TransformerMixin):\r\n",
      "\r\n",
      "    def __init__(self):\r\n",
      "        pass\r\n",
      "\r\n",
      "    def fit(self, X, y=None):\r\n",
      "        return self\r\n",
      "\r\n",
      "    def transform(self, X):\r\n",
      "        dts = pd.Series(X.index).dt\r\n",
      "        X[\"dts_month\"] = dts.month.values\r\n",
      "        X[\"dts_hour\"] = dts.hour.values\r\n",
      "        X[\"dts_day_of_week\"] = dts.dayofweek.values\r\n",
      "\r\n",
      "        return X\r\n",
      "\r\n",
      "from meteocalc import Temp, feels_like\r\n",
      "class FeelsLikeExpander(BaseEstimator, TransformerMixin):\r\n",
      "\r\n",
      "    def __init__(self, temp_col, hum_col, windspeed_col, atemp_col):\r\n",
      "        self.temp = temp_col\r\n",
      "        self.hum = hum_col\r\n",
      "        self.windspeed = windspeed_col\r\n",
      "        self.atemp_col = atemp_col\r\n",
      "\r\n",
      "    def fit(self, X, y=None):\r\n",
      "        return self\r\n",
      "\r\n",
      "    def temp_adjust(self, tup):\r\n",
      "            atemp = feels_like(Temp(tup[self.temp],'c'), tup[self.hum], tup[self.windspeed])\r\n",
      "            tup[self.atemp_col] = atemp.c\r\n",
      "            return tup\r\n",
      "\r\n",
      "    def transform(self,X):\r\n",
      "        X = X.apply(self.temp_adjust, axis=1)\r\n",
      "        X = X.drop([\"temp\",\"hum\",\"windspeed\"], axis=1)\r\n",
      "        return X\r\n",
      "\r\n",
      "\r\n",
      "class LagExpander(BaseEstimator, TransformerMixin):\r\n",
      "\r\n",
      "    def __init__(self, lag_col):\r\n",
      "        self.lag = lag_col\r\n",
      "\r\n",
      "    def fit(self, X, y=None):\r\n",
      "        return self\r\n",
      "\r\n",
      "    def transform(self, X):\r\n",
      "        X[self.lag + \"_lag1\"] = X[self.lag].shift(1, fill_value=X.cnt[0])\r\n",
      "        X[self.lag + \"_lag24\"] = X[self.lag].shift(24, fill_value=X.cnt[0])\r\n",
      "\r\n",
      "        return X\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "class TargetDropper(BaseEstimator, TransformerMixin):\r\n",
      "\r\n",
      "    def __init__(self, target_col):\r\n",
      "        self.target = target_col\r\n",
      "\r\n",
      "    def fit(self, X, y=None):\r\n",
      "        return self\r\n",
      "\r\n",
      "    def transform(self, X):\r\n",
      "#         import pdb; pdb.set_trace() # Here's how to enter the debugger in one line.  Don't forget to delete afterwards!\r\n",
      "        return X.drop(self.target, axis=1)\r\n"
     ]
    }
   ],
   "source": [
    "! cat pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-bedroom",
   "metadata": {},
   "source": [
    "Now the pipeline can be deserialized correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "removed-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = joblib.load(\"train_pipe.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-definition",
   "metadata": {},
   "source": [
    "We can see that the steps from the pipeline are perfectly preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "golden-choice",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jdonaldson/opt/miniconda3/envs/mlearn510/lib/python3.6/site-packages/sklearn/base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('feat_pipe',\n",
       "  Pipeline(steps=[('feat_select',\n",
       "                   FeatureSelector(feature_names=['temp', 'hum', 'windspeed',\n",
       "                                                  'cnt'],\n",
       "                                   ts_index=None)),\n",
       "                  ('feat_dts', DateTimeExpander()),\n",
       "                  ('feat_feels',\n",
       "                   FeelsLikeExpander(atemp_col='atemp', hum_col=None,\n",
       "                                     temp_col=None, windspeed_col=None)),\n",
       "                  ('feat_lag', LagExpander(lag_col=None)),\n",
       "                  ('target_dropper', TargetDropper(target_col=None))])),\n",
       " ('scaler', MinMaxScaler()),\n",
       " ('regressor', LinearRegression())]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-closing",
   "metadata": {},
   "source": [
    "Now we can load in some data for testing the deserialized pipeline.  We don't need to worry about train/test split here... this is just to verify that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "shaped-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"../data/bike-hour-raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-diving",
   "metadata": {},
   "source": [
    "Since the sklearn apis are vectorized, we can request and retrieve many predictions at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hourly-methodology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.30861094,  10.18964081,  19.43346503,  33.97426817,\n",
       "        43.16755554,  37.72667015,  56.25009971,  60.13230485,\n",
       "        80.01827321, 110.57811815])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(dat[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-claim",
   "metadata": {},
   "source": [
    "When we want to make requests against a webserver, we'll need to *serialize* the data on our end in order to transmit it as a web request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-bronze",
   "metadata": {},
   "source": [
    "(Launch server from other notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "single-cathedral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"temp\":{\"0\":3.28},\"hum\":{\"0\":81.0},\"windspeed\":{\"0\":0.0},\"casual\":{\"0\":3},\"registered\":{\"0\":13},\"cnt\":{\"0\":16},\"dtetime\":{\"0\":\"2011-01-01 00:00:00\"}}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialized_input = dat[:1].to_json()\n",
    "serialized_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-ordinary",
   "metadata": {},
   "source": [
    "With properly serialized data, we can pass the payload as *POST* data inside a request, and our server can pick it up from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "consolidated-trail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.308610938795027]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "  \n",
    "url = \"http://127.0.0.1:5000\"\n",
    "response = requests.post(url, data={\"input\": serialized_input})\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-victim",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
